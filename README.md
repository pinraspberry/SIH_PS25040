# FloatChat - AI-Powered Oceanographic Data Platform

> **Smart India Hackathon (SIH) 2024 Submission**

FloatChat is a comprehensive AI-powered platform that transforms complex oceanographic ARGO data into an accessible, interactive system. This solution provides end-to-end data processing, intelligent visualizations, natural language queries, and export capabilities - making ocean science data exploration intuitive for researchers and decision-makers.

## ğŸŒŠ About the Project

ARGO floats collect critical oceanographic data including temperature, salinity, and pressure measurements at various depths across the world's oceans. FloatChat bridges the gap between complex scientific data and user-friendly exploration through:

- **Intelligent Data Processing**: Automated NetCDF ingestion with quality validation
- **Interactive Visualizations**: Geospatial maps and scientific charts
- **AI-Powered Chat Interface**: Natural language queries with contextual responses
- **Comprehensive Export**: Multiple format support (NetCDF, CSV, ASCII)
- **Extensible Architecture**: Ready for BGC, glider, and satellite data integration

## ğŸ¯ Expected Solution Alignment

This implementation delivers the complete solution requirements:

âœ… **End-to-end pipeline**: ARGO NetCDF â†’ PostgreSQL + pgvector storage  
âœ… **Backend LLM system**: RAG-powered natural language to database queries  
âœ… **Frontend dashboard**: Geospatial visualizations (Plotly, Leaflet, Cesium)  
âœ… **Chat interface**: Intent understanding and guided data discovery  
âœ… **Indian Ocean PoC**: Demonstration with extensibility framework  

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        FloatChat Platform                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Frontend Layer                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚Web Dashboardâ”‚ â”‚Chat Interfaceâ”‚ â”‚Visualizationsâ”‚ â”‚Export Tools â”‚â”‚
â”‚  â”‚   (HTML/JS) â”‚ â”‚  (React/JS)  â”‚ â”‚(Plotly/Leaflet)â”‚ â”‚(Multi-format)â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Backend API Layer                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚Flask API    â”‚ â”‚RAG Service  â”‚ â”‚Query Engine â”‚ â”‚Export Serviceâ”‚
â”‚  â”‚Server       â”‚ â”‚             â”‚ â”‚             â”‚ â”‚             â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  AI Services Layer                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚Embedding    â”‚ â”‚LLM Service  â”‚ â”‚Local Models â”‚ â”‚Gemini API   â”‚â”‚
â”‚  â”‚Service      â”‚ â”‚             â”‚ â”‚(Llama)      â”‚ â”‚(Fallback)   â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Data Layer                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚NetCDF       â”‚ â”‚PostgreSQL   â”‚ â”‚Vector       â”‚ â”‚Data Quality â”‚â”‚
â”‚  â”‚Processing   â”‚ â”‚Database     â”‚ â”‚Embeddings   â”‚ â”‚Monitoring   â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âœ¨ Key Features

### ğŸ”„ **Data Processing Pipeline**
- **NetCDF Ingestion**: Automated ARGO float data processing
- **Quality Validation**: Comprehensive QC flag handling and validation
- **Vector Embeddings**: AI-powered semantic search capabilities
- **Batch Processing**: Efficient handling of large datasets

### ğŸ—ºï¸ **Interactive Visualizations**
- **World Map**: Interactive Leaflet.js maps with profile locations
- **Profile Charts**: Temperature-salinity-depth plots with Plotly.js
- **Time Series**: Temporal analysis and trend visualization
- **Heatmaps**: Spatial distribution of oceanographic parameters

### ğŸ’¬ **AI-Powered Chat Interface**
- **Natural Language Queries**: Ask questions in plain English
- **Contextual Responses**: RAG-powered answers with data sources
- **Guided Discovery**: Intelligent suggestions and follow-up questions
- **Intent Understanding**: Smart query interpretation and parameter extraction

### ğŸ“Š **Export & Integration**
- **Multiple Formats**: NetCDF, CSV, ASCII export capabilities
- **Data Reports**: Automated summary and quality reports
- **API Integration**: RESTful APIs for third-party integration
- **Extensible Framework**: Ready for BGC, glider, and satellite data

## ğŸ“‹ Requirements

### System Requirements
- **Python**: 3.8 or higher
- **PostgreSQL**: 17+ with pgvector extension
- **Memory**: 8GB RAM minimum (16GB recommended for AI models)
- **Storage**: 10GB free space for models and data
- **Browser**: Modern web browser with JavaScript support

### Core Dependencies
```
# Backend
Flask>=3.1.0
xarray>=2023.1.0
netcdf4>=1.6.0
sentence-transformers>=2.2.0
psycopg2-binary>=2.9.0
pgvector>=0.2.0
google-generativeai>=0.3.0
plotly>=5.0.0

# Frontend
leaflet>=1.9.0
plotly.js>=2.0.0
bootstrap>=5.0.0
```

### Optional Requirements
- **Gemini API Key**: For cloud-based AI services
- **Local Llama Models**: For offline AI capabilities
- **CUDA Support**: For GPU acceleration (optional)

## ğŸš€ Quick Start

### 1. Environment Setup

```bash
# Clone the repository
git clone https://github.com/pinraspberry/SIH_PS25040.git
cd SIH_PS25040

# Activate virtual environment
source .venv/bin/activate

# Install dependencies
pip install -r backend/requirements.txt
```

### 2. Database Configuration

```bash
# Test PostgreSQL connection
python test_database_connection.py

# Setup vector capabilities
python scripts/run_vector_migration.py

# Validate complete setup
python verify_rag_setup.py
```

### 3. Environment Variables

Create or update `.env` file:
```bash
# Database Configuration
DATABASE_HOST=localhost
DATABASE_PORT=5432
DATABASE_NAME=argo_sih
DATABASE_USER=postgres
POSTGRES_PASSWORD=your_password

# AI Configuration (Optional)
GEMINI_API_KEY=your_gemini_api_key
EMBEDDING_MODEL=sentence-transformers
EMBEDDING_DIMENSION=384
MAX_SEARCH_RESULTS=10

# Application Configuration
FLASK_ENV=development
UPLOAD_FOLDER=data/uploads
MAX_FILE_SIZE=100MB
```

### 4. Launch the Platform

```bash
# Start backend API server
python backend/app.py

# Open web dashboard (when frontend is ready)
open http://localhost:5000

# Process sample data
python ingest_argo.py data/19991021_prof.nc
```

## ğŸ“ Project Structure

```
floatchat-platform/
â”œâ”€â”€ ğŸ“ backend/
â”‚   â”œâ”€â”€ ğŸ“ models/                # Database models and schemas
â”‚   â”œâ”€â”€ ğŸ“ services/              # Business logic services
â”‚   â”œâ”€â”€ ğŸ“ utils/                 # Utility functions
â”‚   â”œâ”€â”€ ğŸ“ routes/                # API route handlers
â”‚   â”œâ”€â”€ ğŸ“„ app.py                 # Main Flask application
â”‚   â””â”€â”€ ğŸ“„ requirements.txt       # Python dependencies
â”œâ”€â”€ ğŸ“ frontend/
â”‚   â”œâ”€â”€ ğŸ“ css/                   # Stylesheets
â”‚   â”œâ”€â”€ ğŸ“ js/                    # JavaScript modules
â”‚   â”œâ”€â”€ ğŸ“ assets/                # Images and static files
â”‚   â””â”€â”€ ğŸ“„ index.html             # Main web interface
â”œâ”€â”€ ğŸ“ config/
â”‚   â””â”€â”€ ğŸ“„ database.py            # Database configuration
â”œâ”€â”€ ï¿½  data/
â”‚   â”œâ”€â”€ ğŸ“ uploads/               # File upload directory
â”‚   â”œâ”€â”€ ğŸ“„ 19991021_prof.nc       # Sample ARGO NetCDF file
â”‚   â””â”€â”€ ğŸ“„ ar_index_global_meta.txt.gz
â”œâ”€â”€ ğŸ“ migrations/
â”‚   â””â”€â”€ ğŸ“„ 001_add_vector_capabilities.sql
â”œâ”€â”€ ï¿½ scrnipts/
â”‚   â”œâ”€â”€ ğŸ“„ run_vector_migration.py
â”‚   â””â”€â”€ ğŸ“„ validate_vector_schema.py
â”œâ”€â”€ ğŸ“„ .env                       # Environment variables
â”œâ”€â”€ ğŸ“„ database_schema.sql        # Base database schema
â””â”€â”€ ğŸ“„ README.md
```

## ï¿½ï¸ Implementation Status

### âœ… **Phase 1: Foundation (Complete)**
- [x] Database schema with pgvector extension
- [x] NetCDF processing pipeline
- [x] Environment configuration and setup
- [x] Project structure and documentation

### ğŸš§ **Phase 2: Backend Services (In Progress)**
- [ ] Flask API server with RESTful endpoints
- [ ] RAG service with embedding and LLM integration
- [ ] Complete ingestion pipeline with batch processing
- [ ] Export service with multiple format support

### ğŸ“‹ **Phase 3: Frontend Dashboard (Planned)**
- [ ] Web dashboard with responsive design
- [ ] Interactive Leaflet.js maps with profile markers
- [ ] Plotly.js charts for scientific visualization
- [ ] Data table with filtering and export

### ğŸ“‹ **Phase 4: Chat Interface (Planned)**
- [ ] Modern chat UI with message history
- [ ] Natural language query processing
- [ ] Intelligent response generation with sources
- [ ] Guided discovery and query suggestions

### ğŸ“‹ **Phase 5: Demo & Deployment (Planned)**
- [ ] Indian Ocean demonstration dataset
- [ ] Performance optimization and testing
- [ ] Documentation and deployment preparation

## ğŸ› ï¸ Available Scripts

### Database Management
```bash
python test_database_connection.py     # Test PostgreSQL connectivity
python scripts/run_vector_migration.py # Add pgvector capabilities
python scripts/validate_vector_schema.py # Validate vector schema
python verify_rag_setup.py            # Complete system verification
```

### Data Processing (Coming Soon)
```bash
python ingest_argo.py <netcdf_file>    # Process NetCDF files
python backend/app.py                  # Start API server
```

### Development Tools
```bash
python test_setup.py                   # Validate development environment
```

## ğŸ¯ Demo Scenarios

### Scenario 1: Data Upload and Processing
1. Upload ARGO NetCDF file through web interface
2. Monitor processing progress and data quality
3. View ingested profiles on interactive map
4. Explore temperature-salinity profiles

### Scenario 2: Natural Language Queries
1. Ask: "Show me temperature profiles near the equator"
2. View AI-generated response with relevant data
3. Explore suggested follow-up questions
4. Export results in preferred format

### Scenario 3: Scientific Analysis
1. Filter profiles by location and date range
2. Compare multiple profiles on interactive charts
3. Identify oceanographic patterns and anomalies
4. Generate comprehensive data reports

## ğŸ” API Endpoints

### Data Endpoints
- `GET /api/profiles` - Retrieve profile data with filtering
- `POST /api/upload` - Upload NetCDF files for processing
- `GET /api/profiles/{id}` - Get specific profile details

### Search & Chat Endpoints
- `POST /api/search` - Semantic search with vector similarity
- `POST /api/chat` - Natural language query processing
- `GET /api/chat/history` - Retrieve chat conversation history

### Visualization Endpoints
- `GET /api/visualize/map` - Map data for geographic visualization
- `GET /api/visualize/profiles` - Chart data for profile visualization

### Export Endpoints
- `POST /api/export/netcdf` - Export data in NetCDF format
- `POST /api/export/csv` - Export data in CSV format
- `POST /api/export/ascii` - Export data in ASCII format

## ğŸ› Troubleshooting

### Common Issues

**Database Connection Problems**
```bash
# Test connection
python test_database_connection.py

# Check PostgreSQL service
brew services start postgresql@17  # macOS
sudo systemctl start postgresql    # Linux
```

**pgvector Extension Issues**
```bash
# Install pgvector extension
psql -d argo_sih -c "CREATE EXTENSION IF NOT EXISTS vector;"

# Verify installation
python scripts/validate_vector_schema.py
```

**Frontend Loading Issues**
- Ensure backend API server is running on port 5000
- Check browser console for JavaScript errors
- Verify CORS configuration in Flask application

**AI Service Issues**
- Check Gemini API key configuration in .env file
- Verify sentence-transformers model download
- Monitor memory usage during embedding computation

## ğŸ“ˆ Performance Considerations

### Database Optimization
- **Vector Indexing**: IVFFlat index for efficient similarity search
- **Query Optimization**: Optimized SQL queries with proper indexing
- **Connection Pooling**: Efficient database connection management
- **Batch Processing**: Optimized bulk data operations

### Frontend Performance
- **Lazy Loading**: Progressive loading of large datasets
- **Caching**: Browser caching for static assets and API responses
- **Compression**: Minified CSS/JS and compressed images
- **Responsive Design**: Optimized for mobile and desktop

### AI Service Performance
- **Model Caching**: Persistent loading of embedding models
- **Batch Embeddings**: Efficient batch processing for multiple texts
- **Fallback Strategy**: Graceful degradation when services are unavailable
- **Memory Management**: Optimized memory usage for large models

## Contributing

This project is developed for Smart India Hackathon 2025. Development guidelines:

1. **Code Style**: Follow PEP 8 for Python, ESLint for JavaScript
2. **Testing**: Add tests for new functionality
3. **Documentation**: Update documentation for new features
4. **Compatibility**: Ensure cross-platform compatibility

## ğŸ“„ License

This project is developed for Smart India Hackathon 2024. Please refer to the competition guidelines for usage and distribution terms.

## Acknowledgments

- **ARGO Program**: For providing open oceanographic data
- **Smart India Hackathon**: For the opportunity to develop this solution
- **Open Source Community**: For the tools and libraries that make this possible

## ğŸš€ Future Roadmap

### Short-term (Next 2-3 months)
- Complete chat interface implementation
- Add real-time data streaming capabilities
- Implement advanced filtering and search
- Mobile application development

### Medium-term (6 months)
- BGC-Argo data integration
- Glider and mooring data support
- Machine learning anomaly detection
- Collaborative features and user management

### Long-term (1 year+)
- Satellite data integration
- Predictive modeling capabilities
- Cloud deployment and scaling
- Integration with national oceanographic systems

---

**Note**: This is a comprehensive platform developed for demonstration purposes. For production deployment, additional security, monitoring, and scalability features should be implemented based on specific operational requirements.